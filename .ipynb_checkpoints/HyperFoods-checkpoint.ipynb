{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "# HyperFoods: Machine intelligent mapping of cancer-beating molecules in foods\n",
    "\n",
    "## Recipe Retrieval w/ Higher Number Anti-Cancer Molecules\n",
    "\n",
    "Each recipe had all the ingredients concatenated in single string. It was used the ingredients vocabulary of the dataset\n",
    "to filter what were and what weren't ingredient names in each string. Finally, it was calculated the sum of the number\n",
    "of anti-cancer molecules present in each recipe using the table food_compound.csv. A DataFrame object was created so that\n",
    "it not ony shows us the ID of each recipe, but also the number of anti-cancer molecules, along with an URL to the recipe's\n",
    "location online."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Importing Modules\n",
    "\n",
    "Importing libraries installed using PyPI and functions present in scripts created in for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------- Data Management ----------------------------\n",
    "# pandas is an open source library providing high-performance, easy-to-use data structures and data \n",
    "# analysis tools for the Python programming language.\n",
    "\n",
    "import pandas\n",
    "\n",
    "# ---------------------------- Scientific Operations ----------------------------\n",
    "# NumPy is the fundamental package for scientific computing with Python. It contains among other things: a powerful \n",
    "# N-dimensional array object, sophisticated (broadcasting) functions, tools for integrating C/C++ and Fortran code, \n",
    "# useful linear algebra, Fourier transform, and random number capabilities.\n",
    "\n",
    "import numpy\n",
    "\n",
    "# ---------------------------- Write & Read JSON Files ----------------------------\n",
    "# Python has a built-in package which can be used to work with JSON data.\n",
    "\n",
    "import json\n",
    "\n",
    "# ---------------------------- Pickling ----------------------------\n",
    "# The pickle module implements binary protocols for serializing and de-serializing a Python object structure. “Pickling”\n",
    "# is the process whereby a Python object hierarchy is converted into a byte stream, and “unpickling” is the inverse \n",
    "# operation, whereby a byte stream (from a binary file or bytes-like object) is converted back into an object hierarchy.\n",
    "\n",
    "import pickle\n",
    "\n",
    "# ------------------------------------- Word2Vec -------------------------------------\n",
    "# Word2Vec is a group of related models that are used to produce word embeddings. These models are shallow, two-layer neural\n",
    "# networks that are trained to reconstruct linguistic contexts of words. Word2vec takes as its input a large corpus of\n",
    "# text and produces a vector space, typically of several hundred dimensions, with each unique word in the corpus being\n",
    "# assigned a corresponding vector in the space. Word vectors are positioned in the vector space such that words that\n",
    "# share common contexts in the corpus are located close to one another in the space.\n",
    "# Gensim is a Python library for topic modelling, document indexing and similarity retrieval with large corpora. Target\n",
    "# audience is the natural language processing (NLP) and information retrieval (IR) community.\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# -------------------------- Dimensionality Reduction Tools --------------------------\n",
    "# Scikit-learn (also known as sklearn) is a free software machine learning library for the\n",
    "# Python programming language.It features various classification, regression and clustering algorithms including \n",
    "# support vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with\n",
    "# the Python numerical and scientific libraries NumPy and SciPy.\n",
    "# Principal component analysis (PCA) - Linear dimensionality reduction using Singular Value Decomposition of the data to\n",
    "# project it to a lower dimensional space. The input data is centered but not scaled for each feature before applying \n",
    "# the SVD.\n",
    "# t-distributed Stochastic Neighbor Embedding (t-SNE) - It is a tool to visualize high-dimensional data. It converts \n",
    "# similarities between data points to joint probabilities and tries to minimize the Kullback-Leibler divergence between\n",
    "# the joint probabilities of the low-dimensional embedding and the high-dimensional data. t-SNE has a cost function that\n",
    "# is not convex, i.e. with different initializations we can get different results.\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# ------------------------------ Check File Existance -------------------------------\n",
    "# The main purpose of the OS module is to interact with the operating system. Its primary use consists in \n",
    "# creating folders, removing folders, moving folders, and sometimes changing the working directory.\n",
    "\n",
    "import os\n",
    "from os import path\n",
    "\n",
    "# ------------------------ Designed Visualization Functions -------------------------\n",
    "# Matplotlib is a Python 2D plotting library which produces publication quality figures in a variety of hardcopy formats\n",
    "# and interactive environments across platforms. Matplotlib can be used in Python scripts, the Python and IPython\n",
    "# shells, the Jupyter notebook, web application servers, and four graphical user interface toolkits.\n",
    "# Plotly's Python graphing library makes interactive, publication-quality graphs. You can use it to make line plots, \n",
    "# scatter plots, area charts, bar charts, error bars, box plots, histograms, heatmaps, subplots, multiple-axes, polar \n",
    "# charts, and bubble charts.\n",
    "# Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing\n",
    "# attractive and informative statistical graphics.\n",
    "\n",
    "from algorithms.view.matplotlib_designed import matplotlib_function\n",
    "from algorithms.view.plotly_designed import plotly_function\n",
    "from algorithms.view.seaborn_designed import seaborn_function\n",
    "\n",
    "# ------------------------ Retrieving Ingredients, Units and Quantities -------------------------\n",
    "\n",
    "from algorithms.parsing.ingredient_quantities import ingredient_quantities\n",
    "\n",
    "# ------------------------ Create Distance Matrix -------------------------\n",
    "# SciPy is a free and open-source Python library used for scientific and technical computing. SciPy contains modules for\n",
    "# optimization, linear algebra, integration, interpolation, special functions, FFT, signal and image processing, ODE\n",
    "# solvers and other tasks common in science and engineering.\n",
    "# distance_matrix returns the matrix of all pair-wise distances.\n",
    "\n",
    "from scipy.spatial import distance_matrix\n",
    "\n",
    "# ------------------------ Clustering Algorithms -------------------------\n",
    "#\n",
    "\n",
    "from clustering.infomapAlgorithm import infomap_function # Infomap algorithm detects communities in large networks with the map equation framework.\n",
    "from sklearn.cluster import DBSCAN # DBSCAN\n",
    "from sklearn.cluster import MeanShift # Meanshift\n",
    "import community # Louvain\n",
    "\n",
    "# ------------------------ Jupyter Notebook Widgets -------------------------\n",
    "# Interactive HTML widgets for Jupyter notebooks and the IPython kernel.\n",
    "\n",
    "import ipywidgets as w\n",
    "from IPython.core.display import display\n",
    "from IPython.display import Image\n",
    "\n",
    "# ------------------------ IoU Score -------------------------\n",
    "# The Jaccard index, also known as Intersection over Union and the Jaccard similarity coefficient (originally given the\n",
    "# French name coefficient de communauté by Paul Jaccard), is a statistic used for gauging the similarity and diversity \n",
    "# of sample sets. The Jaccard coefficient measures similarity between finite sample sets, and is defined as the size of \n",
    "# the intersection divided by the size of the union of the sample sets.\n",
    "# Function implemented during this project.\n",
    "\n",
    "from benchmark.iou_designed import iou_function\n",
    "\n",
    "# ------------------------ F1 Score -------------------------\n",
    "# The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best\n",
    "# value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. The \n",
    "# formula for the F1 score is: F1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# ------------------------ API Requests -------------------------\n",
    "# The requests library is the de facto standard for making HTTP requests in Python. It abstracts the complexities of\n",
    "# making requests behind a beautiful, simple API so that you can focus on interacting with services and consuming data \n",
    "# in your application.\n",
    "\n",
    "import requests\n",
    "\n",
    "# ------------------------ RegEx -------------------------\n",
    "# A RegEx, or Regular Expression, is a sequence of characters that forms a search pattern.\n",
    "# RegEx can be used to check if a string contains the specified search pattern.\n",
    "# Python has a built-in package called re, which can be used to work with Regular Expressions.\n",
    "\n",
    "import re\n",
    "\n",
    "# ------------------------ Inflect -------------------------\n",
    "# Correctly generate plurals, singular nouns, ordinals, indefinite articles; convert numbers to words.\n",
    "\n",
    "import inflect\n",
    "\n",
    "# ------------------------ Parse URLs -------------------------\n",
    "# This module defines a standard interface to break Uniform Resource Locator (URL) strings up in components (addressing\n",
    "# scheme, network location, path etc.), to combine the components back into a URL string, and to convert a “relative URL”\n",
    "# to an absolute URL given a “base URL.”\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# ------------------------ Embedding HTML -------------------------\n",
    "# Public API for display tools in IPython.\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "# ------------------------ Creating Graph -------------------------\n",
    "# NetworkX is a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of \n",
    "# complex networks.\n",
    "\n",
    "import networkx\n",
    "\n",
    "# ------------------------ Language Detectors -------------------------\n",
    "# TextBlob requires API connnection to Google translating tool (low limit on the number of requests). langdetect is an offline detector.\n",
    "\n",
    "from textblob import TextBlob\n",
    "from langdetect import detect\n",
    "\n",
    "# ------------------------ Language Detectors -------------------------\n",
    "# In Python, string.punctuation will give the all sets of punctuation: !\"#$%&'()*+, -./:;<=>?@[\\]^_`{|}~\n",
    "\n",
    "import string\n",
    "\n",
    "# ------------------------ Language Detectors -------------------------\n",
    "# CSV (Comma Separated Values) format is the most common import and export format for spreadsheets and databases.\n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recipe1M+ Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------- Importing Recipe1M+ Dataset ----------------------------\n",
    "\n",
    "f = open('./data/recipe1M+/layer11.json')\n",
    "recipes_data = (json.load(f))#[0:1000]\n",
    "f.close()\n",
    "\n",
    "id_ingredients = {}\n",
    "id_url = {}\n",
    "   \n",
    "for recipe in recipes_data:\n",
    "\n",
    "    id_ingredients[recipe[\"id\"]] = []\n",
    "    id_url[recipe[\"id\"]] = recipe[\"url\"]\n",
    "    \n",
    "    for index, ingredient in enumerate(recipe[\"ingredients\"]):\n",
    "        id_ingredients[recipe[\"id\"]].append({\"id\": index, \"ingredient\": (ingredient[\"text\"]).lower()})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------- Details Recipe1M+ ----------------------------\n",
    "\n",
    "# Online websites parsed to retrieve recipes.\n",
    "\n",
    "recipe_databases = []\n",
    "\n",
    "for key, value in id_url.items():\n",
    "    \n",
    "    parsed_uri = urlparse(value)\n",
    "    result = '{uri.scheme}://{uri.netloc}'.format(uri=parsed_uri)\n",
    "    \n",
    "    recipe_databases.append(result)\n",
    "\n",
    "list(set(recipe_databases)) # The common approach to get a unique collection of items is to use a set. Sets are \n",
    "# unordered collections of distinct objects. To create a set from any iterable, you can simply pass it to the built-in\n",
    "# set() function. If you later need a real list again, you can similarly pass the set to the list() function.\n",
    "\n",
    "with open('./data/allRecipeDatabases.txt', 'w') as f:\n",
    "    for item in list(set(recipe_databases)):\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recipe1M+ Dataset Errors Corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%     \n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ingredients': [{'text': '2 Chicken thighs'}, {'text': '2 tsp Kombu tea'}, {'text': '1 White pepper'}], 'url': 'https://cookpad.com/us/recipes/150100-kombu-tea-grilled-chicken-thigh', 'partition': 'train', 'title': 'Kombu Tea Grilled Chicken Thigh', 'id': '000075604a', 'instructions': [{'text': 'Pierce the skin of the chicken with a fork or knife.'}, {'text': 'Sprinkle with kombu tea evenly on both sides of the chicken, about 1 teaspoon per chicken thigh.'}, {'text': 'Brown the skin side of the chicken first over high heat until golden brown.'}, {'text': 'Sprinkle some pepper on the meat just before flipping over.'}, {'text': 'Then brown the other side until golden brown.'}]}\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------- Deleting Non-English/Empty Recipes ----------------------------\n",
    "\n",
    "true_recipes_positions = []\n",
    "\n",
    "for key, recipe in enumerate(recipes_data):\n",
    "    \n",
    "    joint_ingredients = \"\"\n",
    "    \n",
    "    for key2, ingredient in enumerate(recipe[\"ingredients\"][0:3]):\n",
    "                                                                \n",
    "        #b = TextBlob(modified_recipes_data[key][\"instructions\"][0][\"text\"])\n",
    "        #print(detect(ingredient[\"text\"] + \"a\"))\n",
    "        \n",
    "        joint_ingredients = joint_ingredients + \" \" + ingredient[\"text\"]\n",
    "            \n",
    "        if len(joint_ingredients.split(\" \")) > 5 and detect(ingredient[\"text\"] + \"a\") == \"en\" and key2 == 4:\n",
    "        #if b.detect_language() == \"en\":\n",
    "            #print(\"en\")\n",
    "        \n",
    "            true_recipes_positions.append(key)\n",
    "            \n",
    "print(recipes_data[6])\n",
    "print(true_recipes_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------- Correcting Fractions in Food.com ----------------------------\n",
    "\n",
    "relative_units = {\"cup\": 240, \"cups\": 240, \"c.\": 240, \"tablespoon\": 15, \"tablespoons\": 15, \"bar\": 150, \"bars\": 150, \"lump\": 5, \"lumps\": 5, \"piece\": 25, \"pieces\": 25, \"portion\": 100, \"portions\": 100, \"slice\": 10, \"slices\": 10, \"teaspoon\": 5, \"teaspoons\": 5, \"tbls\": 15, \"tsp\": 5, \"jar\": 250, \"jars\": 250, \"pinch\": 1, \"pinches\": 1, \"dash\": 1, \"can\": 330, \"box\": 250, \"boxes\": 250, \"small\": 250, \"medium\": 500, \"large\": 750, \"big\": 750, \"sprig\": 0.1, \"sprigs\": 0.1, \"bunch\": 100, \"bunches\": 100, \"leaves\": 0.1, \"packs\": 100, \"packages\": 100, \"pck\": 100, \"pcks\": 100, \"stalk\": 0.1}\n",
    "\n",
    "modified_recipes_data = original_recipes_data\n",
    "\n",
    "#print(original_recipes_data)\n",
    "\n",
    "for key, recipe in enumerate(original_recipes_data):\n",
    "    \n",
    "    if (\".food.com\" or \"/food.com\") in recipe[\"url\"]:\n",
    "        \n",
    "        for key2, ingredient in enumerate(recipe[\"ingredients\"]):\n",
    "            \n",
    "            if re.search(r\"[1-5][1-9]\", ingredient[\"text\"]):\n",
    "                \n",
    "                number = re.search(r\"[1-5][1-9]\", ingredient[\"text\"]).group()\n",
    "                \n",
    "                split_ingredient_list = (ingredient[\"text\"].split(\" \"))\n",
    "                \n",
    "                for index in range(len(split_ingredient_list) - 1):\n",
    "                                        \n",
    "                    if split_ingredient_list[index] == number and split_ingredient_list[index + 1] in list(relative_units.keys()):\n",
    "                        \n",
    "                        split_ingredient = split_ingredient_list[index][0] + \"/\" + split_ingredient_list[index][1]\n",
    "                        split_ingredient = \"\".join(split_ingredient)\n",
    "                        \n",
    "                        split_ingredient_list[index] = split_ingredient\n",
    "                        split_ingredient_list = \" \".join(split_ingredient_list)\n",
    "                        \n",
    "                        modified_recipes_data[key][\"ingredients\"][key2][\"text\"] = split_ingredient_list\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------- Exporting Corrected Recipe Dataset ----------------------------\n",
    "\n",
    "with open('./data/recipe1M+/modified_modified_recipes_data_mod.json', 'w') as json_file:\n",
    "    \n",
    "    json.dump(modified_modified_recipes_data, json_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    print(detect(\"m 1, 5 . . ( )\"))\n",
    "    \n",
    "except ValueError:\n",
    "    print(\"wrong\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Creating Units Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "p = inflect.engine()\n",
    "\n",
    "with open('./vocabulary/ingr_vocab.pkl', 'rb') as f: # Includes every ingredient present in the dataset.\n",
    "    ingredients_list = pickle.load(f)\n",
    "\n",
    "f = open('./data/recipe1M+/layer11.json')\n",
    "original_recipes_data = (json.load(f))#[0:100000]\n",
    "f.close()\n",
    "\n",
    "units_list_temp = set()\n",
    "\n",
    "def get_units(ingredient_text_input, number_input):\n",
    "    \n",
    "    split_ingredient_list2 = ingredient_text_input.replace(\"/\", \" \").replace(\"-\", \" \").translate({ord(ii): None for ii in string.punctuation.replace(\".\", \"\")}).lower().split(\" \")\n",
    "    print(split_ingredient_list2)\n",
    "    \n",
    "    for number_input_it in number_input:\n",
    "        \n",
    "        for iji in range(len(split_ingredient_list2) - 1):\n",
    "                                            \n",
    "            if split_ingredient_list2[iji] == number_input_it and re.search(r\"[0-9]\", split_ingredient_list2[iji + 1]) is None and re.search(r\".\\b\", split_ingredient_list2[iji + 1]) is None:\n",
    "                            \n",
    "                units_list_temp.add(split_ingredient_list2[iji + 1])\n",
    "                break\n",
    "\n",
    "for original_recipes_data_it in original_recipes_data:\n",
    "        \n",
    "        for ingredient_it in original_recipes_data_it[\"ingredients\"]:\n",
    "                        \n",
    "            # search_number = re.search(r\"\\d\", ingredient_text)\n",
    "            \n",
    "            number_array = re.findall(r\"\\d\", ingredient_it[\"text\"])\n",
    "            \n",
    "            if number_array:\n",
    "                \n",
    "                # search_number.group() # [0-9]|[0-9][0-9]|[0-9][0-9][0-9]|[0-9][0-9][0-9][0-9]                \n",
    "                get_units(ingredient_it[\"text\"], number_array)\n",
    "                \n",
    "units_list = list(units_list_temp)\n",
    "units_list.sort()\n",
    "\n",
    "print(units_list)\n",
    "\n",
    "# Save a dictionary into a txt file.\n",
    "with open('./vocabulary/units_list.txt', 'w') as f:\n",
    "    for item in units_list:\n",
    "        if item != \"<end>\" and item != \"<pad>\":\n",
    "            f.write(\"%s\\n\" % item)\n",
    "            \n",
    "#for jj, ingredients_list_it in enumerate(ingredients_list):\n",
    "                    \n",
    "                    #if predicted_unit in ingredients_list_it or predicted_unit in p.plural(ingredients_list_it):\n",
    "                \n",
    "                        #break\n",
    "                \n",
    "                    #elif jj == len(ingredients_list) - 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hey = [0, 4, 1, 4, 9]\n",
    "\n",
    "print(set(hey))\n",
    "\n",
    "print(0 in set(hey))\n",
    "\n",
    "for e in set(hey):\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "p = inflect.engine()\n",
    "\n",
    "with open('./vocabulary/ingr_vocab.pkl', 'rb') as f: # Includes every ingredient present in the dataset.\n",
    "    ingredients_list = pickle.load(f)  \n",
    "\n",
    "lineList = [line.rstrip('\\n') for line in open('./vocabulary/units_list.txt')]\n",
    "  \n",
    "print(lineList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "final_units = []\n",
    "\n",
    "for unit in lineList:\n",
    "\n",
    "    for index, ingredients_list_it in enumerate(ingredients_list):\n",
    "                        \n",
    "        if unit == ingredients_list_it or unit == p.plural(ingredients_list_it):\n",
    "                    \n",
    "            break\n",
    "                    \n",
    "        elif index == len(ingredients_list) - 1:\n",
    "            \n",
    "            final_units.append(unit)\n",
    "\n",
    "print(len(final_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Save a dictionary into a txt file.\n",
    "with open('./vocabulary/units_list_final.txt', 'w') as f:\n",
    "    for item in final_units:\n",
    "        if item != \"<end>\" and item != \"<pad>\":\n",
    "            f.write(\"%s\\n\" % item)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download()\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "import webcolors\n",
    "\n",
    "food = wordnet.synset('food.n.02')\n",
    "\n",
    "print(\"red\" in webcolors.CSS3_NAMES_TO_HEX)\n",
    "\n",
    "with open(\"./vocabulary/units_list_final - cópia.txt\") as f:\n",
    "    content = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "lines = [x.strip() for x in content] \n",
    "\n",
    "filtered_stopwords = [word for word in lines if word not in stopwords.words('english')]\n",
    "filtered_verbs_adjectives_adverbs = []\n",
    "\n",
    "for w in filtered_stopwords:\n",
    "    if wordnet.synsets(w) and wordnet.synsets(w)[0].pos() != \"v\" and wordnet.synsets(w)[0].pos() != \"a\" and wordnet.synsets(w)[0].pos() != \"r\" and w not in webcolors.CSS3_NAMES_TO_HEX and w not in list(set([w for s in food.closure(lambda s:s.hyponyms()) for w in s.lemma_names()])):\n",
    "        filtered_verbs_adjectives_adverbs.append(w)\n",
    "    elif wordnet.synsets(w) == []:\n",
    "        filtered_verbs_adjectives_adverbs.append(w)\n",
    "\n",
    "print(filtered_stopwords)\n",
    "print(len(lines))\n",
    "print(len(filtered_stopwords))\n",
    "print(len(filtered_verbs_adjectives_adverbs))\n",
    "\n",
    "# Save a dictionary into a txt file.\n",
    "with open('./vocabulary/units_list_final_filtered.txt', 'w') as f:\n",
    "    for item in filtered_verbs_adjectives_adverbs:\n",
    "        if item != \"<end>\" and item != \"<pad>\":\n",
    "            f.write(\"%s\\n\" % item)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "food = wn.synset('food.n.02')\n",
    "len(list(set([w for s in food.closure(lambda s:s.hyponyms()) for w in s.lemma_names()])))\n",
    "list(set([w for s in food.closure(lambda s:s.hyponyms()) for w in s.lemma_names()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "b = TextBlob(\"En una procesadora o batidora mescla el queso crema, pistachos, y 1 de los ajos\")\n",
    "b.detect_language()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Retrieving Ingredients, Units and Quantities from Recipe1M+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------- Creating Vocabulary to Import Units ----------------------------\n",
    "\n",
    "absolute_units = {\"litre\": 1000, \"litres\": 1000, \"ounce\": 28, \"ounces\": 28, \"gram\": 1, \"grams\": 1, \"grm\": 1, \"kg\": 1000, \"kilograms\": 1000, \"ml\": 1, \"millilitres\": 1, \"oz\": 28, \"l\": 1000, \"g\": 1, \"lbs\": 454, \"pint\": 568, \"pints\": 568, \"lb\": 454, \"gallon\": 4546, \"gal\": 4546, \"quart\": 1137, \"quarts\": 1137}\n",
    "relative_units = {\"cup\": 240, \"cups\": 240, \"c.\": 240, \"tablespoon\": 15, \"tablespoons\": 15, \"bar\": 150, \"bars\": 150, \"lump\": 5, \"lumps\": 5, \"piece\": 25, \"pieces\": 25, \"portion\": 100, \"portions\": 100, \"slice\": 10, \"slices\": 10, \"teaspoon\": 5, \"teaspoons\": 5, \"tbls\": 15, \"tsp\": 5, \"jar\": 250, \"jars\": 250, \"pinch\": 1, \"pinches\": 1, \"dash\": 1, \"can\": 330, \"box\": 250, \"boxes\": 250, \"small\": 250, \"medium\": 500, \"large\": 750, \"big\": 750, \"sprig\": 0.1, \"sprigs\": 0.1, \"bunch\": 100, \"bunches\": 100, \"leaves\": 0.1, \"packs\": 100, \"packages\": 100, \"pck\": 100, \"pcks\": 100, \"stalk\": 0.1}\n",
    "\n",
    "# ---------------------------- Save a dictionary into a txt file ----------------------------\n",
    "\n",
    "with open('./vocabulary/absolute_units.json', 'w') as json_file:\n",
    "    json.dump(absolute_units, json_file)\n",
    "    \n",
    "with open('./vocabulary/relative_units.json', 'w') as json_file:\n",
    "    json.dump(relative_units, json_file)\n",
    "    \n",
    "# ---------------------------- Importing and Exporting as Text File Ingredient's Vocabulary ----------------------------\n",
    "\n",
    "# Reading ingredients vocabulary.\n",
    "# with open('./vocabulary/instr_vocab.pkl', 'rb') as f: # Includes every ingredient, cooking vocabulary and punctuation signals necessary to describe a recipe in the dataset.\n",
    "with open('./vocabulary/ingr_vocab.pkl', 'rb') as f: # Includes every ingredient present in the dataset.\n",
    "    ingredients_list = pickle.load(f) # Using vocabulary ingredients to retrieve the ones present in the recipes.\n",
    "    \n",
    "# Save a dictionary into a txt file.\n",
    "with open('./vocabulary/ingr_vocab.txt', 'w') as f:\n",
    "    for item in ingredients_list:\n",
    "        if item != \"<end>\" and item != \"<pad>\":\n",
    "            f.write(\"%s\\n\" % item)\n",
    "    \n",
    "# ---------------------------- Importing Ingredients, Units and Quantities ----------------------------\n",
    "    \n",
    "relative_units.update(absolute_units)\n",
    "units_list_dict = relative_units\n",
    "\n",
    "ingrs_quants_units_final = {}\n",
    "\n",
    "for recipe in recipes_data:\n",
    "    \n",
    "    ingrs_quants_units_final[recipe[\"id\"]] = ingredient_quantities(recipe, ingredients_list, units_list_dict)\n",
    "        \n",
    "# Exporting data for testing\n",
    "#with open('./data/test/new_id_ingredients_tokenized_position.json', 'w') as json_file:\n",
    "    #json.dump(new_id_ingredients_tokenized_position, json_file)\n",
    "    \n",
    "#with open('./data/test/id_ingredients.json', 'w') as json_file:\n",
    "    #json.dump(id_ingredients, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'000018c8a5': ['penne', 'cheese', 'cheese', 'gruyere', 'chili', 'butter', 'stick', 'flour', 'milk', 'cheese', 'cheese', 'salt', 'chili', 'garlic'], '000033e39b': ['macaroni', 'cheese', 'celery', 'pepper', 'greens', 'pimentos', 'mayonnaise', 'salad dressing', 'vinegar', 'salt', 'dill'], '000035f7ed': ['tomato', 'salt', 'onion', 'pepper', 'greens', 'pepper', 'pepper', 'cucumber', 'oil', 'olive', 'basil'], '00003a70b1': ['milk', 'water', 'butter', 'potato', 'corn', 'cheese', 'onion'], '00004320bb': ['gelatin', 'watermelon', 'water', 'cool whip', 'watermelon', 'cracker'], '0000631d90': ['coconut', 'beef', 'garlic', 'salt', 'pepper', 'juice', 'lemon', 'soy sauce', 'cornstarch', 'pineapple', 'liquid', 'orange', 'liquid', 'nuts', 'cashews'], '000075604a': ['chicken', 'tea', 'kombu', 'pepper'], '00007bfd16': ['rhubarb', 'rhubarb', 'sugar', 'gelatin', 'strawberry', 'strawberries', 'cake', 'water', 'butter', 'margarine'], '000095fc1d': ['vanilla', 'fat', 'yogurt', 'strawberry', 'strawberries', 'fat'], '0000973574': ['flour', 'cinnamon', 'baking soda', 'salt', 'baking powder', 'egg', 'sugar', 'oil', 'vegetables', 'vanilla', 'zucchini', 'walnuts'], '0000a4bcf6': ['onion', 'greens', 'pepper', 'salmon', 'fillets', 'steak', 'oil', 'olive', 'ginger', 'rice', 'greens', 'mixed salad green', 'carrot', 'cherries', 'tomato', 'fat', 'italian dressing', 'cheese', 'banana', 'kiwi', 'vanilla', 'yogurt', 'cinnamon', 'sugar'], '0000b1e2b5': ['seeds', 'fennel', 'tenderloin', 'pork', 'fennel', 'oil', 'olive', 'garlic', 'clove', 'wine', 'chicken', 'broth', 'butter', 'juice', 'lemon'], '0000c79afb': ['wine', 'roses', 'brandy', 'liqueur', 'orange', 'grand marnier', 'juice', 'cranberries', 'orange', 'lemon', 'sprite', 'ice'], '0000ed95f8': ['butter', 'sugar', 'egg', 'juice', 'orange', 'orange', 'flour', 'baking soda', 'salt', 'juice', 'pineapple', 'pecans'], '00010379bf': ['cake', 'flour', 'baking powder', 'sugar', 'seeds', 'sesame', 'water', 'oil', 'vegetables', 'sugar', 'candy', 'candies', 'sugar', 'candy', 'candies', 'soy sauce', 'candy', 'candies', 'water', 'candy', 'candies'], '000106ec3c': ['tomato', 'corn', 'cheese', 'medium cheddar', 'potato', 'onion', 'hamburger'], '00010c7867': ['beef', 'oats', 'juice', 'tomato', 'egg', 'salt', 'pepper', 'chili', 'onion', 'butter', 'flour', 'milk', 'cheese', 'corn', 'pepper', 'greens'], '00010d44c7': ['broccoli', 'rice', 'cheese', 'cheese', 'egg', 'butter', 'milk', 'onion', 'garlic', 'basil', 'oregano', 'salt', 'pepper'], '00011e0b2c': ['marinade', 'beef', 'steak', 'sirloin', 'asparagus', 'flour', 'tortilla'], '00011fc1f9': ['lentils', 'onion', 'tomato', 'carrot', 'celery', 'oil', 'vegetables', 'olive', 'garlic', 'juice', 'ginger', 'vinaigrette', 'broth', 'vegetables', 'pepper', 'chili', 'berbere', 'water', 'salt', 'pepper'], '000128a538': ['vanilla', 'milk', 'blueberries', 'oats', 'coconut', 'pecans', 'vanilla', 'yogurt'], '00013266c9': ['cracker', 'saltine', 'butter', 'sugar', 'vanilla', 'extract', 'nuts'], '00015b5a39': ['potato', 'beef', 'broth', 'water', 'wine', 'teriyaki sauce', 'garlic', 'cream'], '00016355e6': ['vanilla', 'cookie', 'wafers', 'banana', 'coconut', 'milk', 'fat', 'vanilla', 'extract', 'stevia', 'ice', 'cream'], '0001678f7a': ['rice', 'fillets', 'halibut', 'foie gras', 'chives', 'salt', 'pepper', 'chives', 'oil', 'truffle', 'foie gras', 'egg', 'cream', 'oil', 'truffle', 'chicken', 'chives', 'salt', 'pepper', 'spaghetti', 'squash', 'butter', 'honey', 'ginger', 'salt', 'pepper'], '00016d71a4': ['wafers', 'vanilla', 'ice', 'cream', 'lemonade', 'chips', 'chocolate', 'butter', 'margarine'], '00018371f2': ['juice', 'raspberries', 'pectin', 'certo', 'sugar'], '0001960f61': ['cinnamon', 'bread', 'raisins', 'cheese', 'cream', 'apple', 'egg', 'cream', 'butter', 'syrup'], '00019675ca': ['crabmeat', 'cream', 'cheese', 'cream', 'cheese', 'bacon', 'cilantro', 'seasoning', 'garlic', 'onion', 'pepper', 'salt'], '0001a2f336': ['sausage', 'cheese', 'seasoning', 'salt', 'garlic', 'spaghetti', 'jar', 'oil', 'olive', 'pizza dough'], '0001bdeec0': ['greens', 'leek', 'bacon', 'butter', 'salt', 'pepper', 'potato', 'garlic', 'clove', 'thyme', 'egg', 'cheese', 'nutmeg'], '0001cba765': ['juice', 'orange', 'tequila', 'triple sec', 'juice', 'cranberries', 'orange', 'sugar'], '0001d356b6': ['sugar', 'butter', 'stick', 'egg', 'lemon', 'juice', 'lemon', 'flour', 'baking powder', 'baking soda', 'salt', 'buttermilk', 'blueberries', 'sugar', 'butter', 'stick', 'cheese', 'cream', 'sugar', 'vanilla', 'extract', 'salt'], '0001d6acb7': ['butter', 'flour', 'milk', 'cheese', 'chile', 'poblano chiles', 'salt', 'pepper', 'cilantro', 'mustard', 'mustard', 'grain', 'honey', 'water', 'sugar', 'yeast', 'butter', 'salt', 'flour', 'oil', 'vegetables', 'water', 'baking soda', 'water', 'egg', 'salt'], '0001d81db6': ['avocado', 'garlic', 'clove', 'cheese', 'cream', 'juice', 'lime', 'cilantro', 'cream', 'jalapeno', 'tomato'], '000238353f': ['sausage', 'pork', 'meat', 'onion', 'garlic', 'clove', 'thyme', 'marmalade', 'puff pastry', 'egg', 'seeds', 'poppy seed', 'salt', 'pepper', 'oil', 'butter'], '0002491373': ['cooking spray', 'pepper', 'greens', 'pepper', 'pepper', 'onion', 'garlic', 'clove', 'eggplant', 'pepper', 'jalapeno', 'cilantro', 'capers', 'currants', 'nuts', 'vinegar', 'wine', 'salt', 'pepper'], '00025af750': ['pepper', 'banana', 'tomato', 'pepper', 'scallion', 'cucumber', 'basil', 'tarragon', 'stock', 'vegetables', 'salt', 'pepper', 'worcestershire sauce', 'hot sauce', 'vinegar', 'wine', 'vodka'], '00027b61de': ['lobster', 'butter', 'juice', 'lemon', 'salt', 'garlic'], '00029df38f': ['egg', 'salt', 'sugar', 'flour', 'cinnamon', 'vanilla', 'extract', 'pecans'], '00029f71f7': ['flour', 'wheat', 'oats', 'sugar', 'parsley', 'apple', 'carrot', 'egg', 'oil', 'vegetables', 'water'], '0002a82634': ['popcorn', 'fudge', 'caramel'], '0002e15d76': ['onion', 'oil', 'ketchup', 'water', 'vinegar', 'cider', 'sugar', 'worcestershire sauce', 'honey', 'mustard', 'paprika', 'salt', 'pepper', 'pepper', 'lemon', 'sausage', 'hot dog'], '0002ed1338': ['sugar', 'pumpkin', 'spices', 'cinnamon', 'biscuit', 'egg', 'water', 'sugar', 'syrup'], '0003132d05': ['chives', 'potato', 'garlic', 'clove', 'butter', 'milk', 'cream', 'cheese', 'salt', 'salt', 'onion', 'garlic', 'pepper'], '000320b7ce': ['flour', 'baking powder', 'baking soda', 'salt', 'cinnamon', 'ginger', 'sugar', 'butter', 'banana', 'egg', 'juice', 'lime', 'lime', 'coconut'], '000328f1ed': ['penne', 'sugar', 'peas', 'cream', 'trout', 'lemon', 'dill', 'salt', 'pepper'], '00032d5bcd': ['water', 'juice', 'orange', 'sugar', 'fruit', 'dates', 'cinnamon', 'stick', 'cheese', 'cream', 'philadelphia', 'fat', 'milk', 'honey', 'almonds'], '00033f624d': ['butter', 'egg', 'cheese', 'colby', 'pepper', 'salt', 'curry', 'flour', 'fat', 'bacon', 'ham', 'cheese'], '00034ad6cc': ['pasta', 'fettuccine', 'butter', 'garlic', 'clove', 'cream', 'egg', 'cheese', 'parsley']}\n"
     ]
    }
   ],
   "source": [
    "new_id_ingredients_tokenized = {}\n",
    "\n",
    "for key, value in ingrs_quants_units_final.items():\n",
    "    \n",
    "    new_id_ingredients_tokenized[key] = []\n",
    "    \n",
    "    for value2 in value:\n",
    "        \n",
    "        new_id_ingredients_tokenized[key].append(value2[\"ingredient\"])\n",
    "        \n",
    "print(new_id_ingredients_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving Cooking Processes from Recipe1M+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingredients -> Vector (Word2Vec)\n",
    "\n",
    "Converting ingredients into 50 dimensional vectors to facilitate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Ingredients are converted into vectors and, by averaging the ones belonging to the same recipe, a vector for the\n",
    "# recipe is obtained.\n",
    "\n",
    "if path.exists(\"./trained_models/model.bin\"):\n",
    "    \n",
    "    corpus = new_id_ingredients_tokenized.values()\n",
    "    \n",
    "    model = Word2Vec(corpus, min_count=1,size= 50,workers=3, window =10, sg = 0)\n",
    "\n",
    "    words = list(model.wv.vocab)\n",
    "\n",
    "# By default, the model is saved in a binary format to save space.\n",
    "    model.wv.save_word2vec_format('./trained_models/model.bin')\n",
    "\n",
    "# Save the learned model in ASCII format and review the contents\n",
    "    model.wv.save_word2vec_format('./trained_models/model.txt', binary=False)\n",
    "\n",
    "else:\n",
    "    \n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format('./trained_models/model.bin', binary=True) # Saved model can then be loaded again by calling the Word2Vec.load() function.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingredients -> Vector (Every vector component corresponds to a word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recipes -> Vector (Word2Vec)\n",
    "\n",
    "Representing recipes in their vectorized way by taking the average of the vectors of the ingredients present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "new_id_ingredients_tokenized_keys = new_id_ingredients_tokenized.keys()\n",
    "\n",
    "id_ingreVectorized = {}\n",
    "id_recipe = {}\n",
    "\n",
    "for recipe_id in new_id_ingredients_tokenized_keys:\n",
    "    \n",
    "    id_ingreVectorized[recipe_id] = []\n",
    "    \n",
    "    for recipe_ingr in new_id_ingredients_tokenized[recipe_id]:\n",
    "        \n",
    "        id_ingreVectorized[recipe_id].append(model[recipe_ingr])\n",
    "\n",
    "    id_recipe[recipe_id] = sum(id_ingreVectorized[recipe_id])/len(new_id_ingredients_tokenized[recipe_id])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recipes -> Vector (Every vector component corresponds to a word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction (Ingredients)\n",
    "\n",
    "PCA and T-SNE intended to decrease the dimensionality (50) of the vectors representing ingredients, so that they can be \n",
    "plotted in visualizable way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_ingredients = model[model.wv.vocab]\n",
    "\n",
    "print(X_ingredients)\n",
    "\n",
    "# ---------------------------- PCA ----------------------------\n",
    "X_ingredients_embedded1 = PCA(n_components=2).fit_transform(X_ingredients)\n",
    "\n",
    "# ---------------------------- T-SNE ----------------------------\n",
    "X_ingredients_embedded2 = TSNE(n_components=2).fit_transform(X_ingredients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Ingredients\n",
    "\n",
    "Finding groups of ingredients that most often co-occur in the same recipes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------- Build Distance Dataframe & Networkx Graph ----------------------------\n",
    "\n",
    "data = list(X_ingredients_embedded1) # list(X_ingredients_embedded1) / model[model.wv.vocab]\n",
    "ctys = list(model.wv.vocab)\n",
    "df = pandas.DataFrame(data, index=ctys)\n",
    "\n",
    "distances = (pandas.DataFrame(distance_matrix(df.values, df.values), index=df.index, columns=df.index)).rdiv(1) # Creating dataframe from distance matrix between ingredient vectors.\n",
    "# G = networkx.from_pandas_adjacency(distances) # Creating networkx graph from pandas dataframe.\n",
    "X = numpy.array(df.values) # Creating numpy array from pandas dataframe.\n",
    "\n",
    "# ---------------------------- Clustering ----------------------------\n",
    "\n",
    "# Mean Shift\n",
    "\n",
    "#  ingredientModule = MeanShift().fit(X).labels_\n",
    "\n",
    "# Density-Based Spatial Clustering of Applications with Noise (DBSCAN)\n",
    "\n",
    "# ingredientModule = DBSCAN(eps=0.3, min_samples=2).fit(X).labels_ # Noisy samples are given the label -1.\n",
    "\n",
    "# Louvain\n",
    "\n",
    "# ingredientModule = list((community.best_partition(G)).values())\n",
    "\n",
    "# Infomap\n",
    "\n",
    "ingredientModule = infomap_function(distances, ctys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Times Ingredients are used in Recipes\n",
    "\n",
    "Retrieving how often different ingredients are used across the recipe dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ingredients_count = {}\n",
    "\n",
    "for ingredient in ingredients_list:\n",
    "\n",
    "    if \"_\" in ingredient:\n",
    "        ingredients_count[ingredient.replace(\"_\", \" \")] = 0\n",
    "        continue\n",
    "\n",
    "    ingredients_count[ingredient] = 0 # In case there is no _\n",
    "\n",
    "for recipe in recipes_data:\n",
    "        \n",
    "    for recipe_standardized in ingrs_quants_units_final[recipe[\"id\"]]:\n",
    "        \n",
    "        ingredients_count[recipe_standardized[\"ingredient\"]] = ingredients_count[recipe_standardized[\"ingredient\"]] + recipe_standardized[\"quantity\"]\n",
    "\n",
    "# -------------------------------\n",
    "\n",
    "ingredientSize = {}\n",
    "markerSizeConstant = 1\n",
    "\n",
    "for ingredient_vocabulary in list(model.wv.vocab):\n",
    "    \n",
    "    ingredientSize[ingredient_vocabulary] = markerSizeConstant*ingredients_count[ingredient_vocabulary]\n",
    "    \n",
    "ingredientSize = list(ingredientSize.values())\n",
    "\n",
    "print(ingredientSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  PCA & T-SNE Visualization (Ingredients)\n",
    "\n",
    "Although some informamation was inevitably lost, a pair of the most variable components was used. <br>\n",
    "Size of each marker is proportional to the number of times the ingredient is used in the recipe dataset. <br>\n",
    "Markers with a similar color group ingredients that are usually used together in the recipe dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------- Matplotlib ----------------------------\n",
    "matplotlib_function(X_ingredients_embedded1, X_ingredients_embedded2, list(model.wv.vocab), ingredientModule, ingredientSize, \"Ingredients\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------- Plotly ----------------------------\n",
    "plotly_function(X_ingredients_embedded1, X_ingredients_embedded2, list(model.wv.vocab), ingredientModule, ingredientSize, \"true\", \"Ingredients\")\n",
    "\n",
    "# Toggle Button for Labels\n",
    "toggle = w.ToggleButton(description='No Labels')\n",
    "out = w.Output(layout=w.Layout(border = '1px solid black'))\n",
    "\n",
    "def fun(obj):\n",
    "    with out:\n",
    "        if obj['new']:  \n",
    "            plotly_function(X_ingredients_embedded1, X_ingredients_embedded2, list(model.wv.vocab), ingredientModule, ingredientSize, \"false\")\n",
    "        else:\n",
    "            plotly_function(X_ingredients_embedded1, X_ingredients_embedded2, list(model.wv.vocab), ingredientModule, ingredientSize, \"true\")\n",
    "\n",
    "toggle.observe(fun, 'value')\n",
    "display(toggle)\n",
    "display(out)\n",
    "\n",
    "# (Run in localhost to visualize it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------- Seaborn ----------------------------\n",
    "seaborn_function(X_ingredients_embedded1, X_ingredients_embedded2, list(model.wv.vocab), ingredientModule, ingredientSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction (Recipes)\n",
    "\n",
    "PCA and T-SNE intended to decrease the dimensionality (50) of the vectors representing recipes, so that they can be \n",
    "plotted in visualizable way. Although some informamation was inevitably lost, a pair of the most variale components was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------- PCA ----------------------------\n",
    "X_recipes_embedded1 = PCA(n_components=2).fit_transform(list(id_recipe.values()))\n",
    "\n",
    "# ---------------------------- T-SNE ----------------------------\n",
    "X_recipes_embedded2 = TSNE(n_components=2).fit_transform(list(id_recipe.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Recipes\n",
    "\n",
    "Finding groups of recipes that most correspond to different types of cuisine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------- Build Distance Dataframe & Networkx Graph ----------------------------\n",
    "\n",
    "data = list(X_recipes_embedded1) # list(X_recipes_embedded1) / id_recipe.values()\n",
    "ctys = id_recipe.keys()\n",
    "df = pandas.DataFrame(data, index=ctys)\n",
    "\n",
    "distances = (pandas.DataFrame(distance_matrix(df.values, df.values), index=df.index, columns=df.index)).rdiv(1)\n",
    "# G = networkx.from_pandas_adjacency(distances) # Creating networkx graph from pandas dataframe.\n",
    "X = numpy.array(df.values) # Creating numpy array from pandas dataframe.\n",
    "\n",
    "# ---------------------------- Clustering ----------------------------\n",
    "\n",
    "# Mean Shift\n",
    "\n",
    "recipeModules = MeanShift().fit(X).labels_\n",
    "\n",
    "# Density-Based Spatial Clustering of Applications with Noise (DBSCAN)\n",
    "\n",
    "# recipeModules = DBSCAN(eps=0.3, min_samples=2).fit(X).labels_ # Noisy samples are given the label -1.\n",
    "\n",
    "# Louvain\n",
    "\n",
    "# recipeModules = list((community.best_partition(G)).values())\n",
    "\n",
    "# Infomap\n",
    "\n",
    "# recipeModules = infomap_function(1./distances, ctys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Ingredients in each Recipe\n",
    "\n",
    "Calculated so that the size of each recipe marker could be proportional to the number of ingredients present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "numberIngredients = []\n",
    "markerSizeConstant = 1\n",
    "\n",
    "for key, value in new_id_ingredients_tokenized.items():\n",
    "    \n",
    "    numberIngredients.append(markerSizeConstant*len(value))\n",
    "\n",
    "print(numberIngredients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA & T-SNE Visualization\n",
    "\n",
    "Size of each marker is proportional to the number of ingredients a given recipe contains. <br>\n",
    "Markers with a similar color group recipes that contain the higher number of common ingredients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------- Matplotlib ----------------------------\n",
    "matplotlib_function(X_recipes_embedded1, X_recipes_embedded2, list(id_recipe.keys()), recipeModules, numberIngredients, \"Recipes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------- Plotly ----------------------------\n",
    "plotly_function(X_recipes_embedded1, X_recipes_embedded2, list(id_recipe.keys()), recipeModules, numberIngredients, \"true\", \"Recipes\")\n",
    "\n",
    "toggle = w.ToggleButton(description='No Labels')\n",
    "\n",
    "out = w.Output(layout=w.Layout(border = '1px solid black'))\n",
    "\n",
    "def fun(obj):\n",
    "    with out:\n",
    "        if obj['new']:  \n",
    "            plotly_function(X_recipes_embedded1, X_recipes_embedded2, list(id_recipe.keys()), recipeModules, numberIngredients, \"false\")\n",
    "        else:\n",
    "            plotly_function(X_recipes_embedded1, X_recipes_embedded2, list(id_recipe.keys()), recipeModules, numberIngredients, \"true\")\n",
    "\n",
    "toggle.observe(fun, 'value')\n",
    "display(toggle)\n",
    "display(out)\n",
    "\n",
    "# (Run in localhost to be able to visualize it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------- Seaborn ----------------------------\n",
    "seaborn_function(X_recipes_embedded1, X_recipes_embedded2, list(id_recipe.keys()), recipeModules, numberIngredients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Anticancer Ingredients\n",
    "\n",
    "Getting the anticancer ingredients and the number of anticancer molecules each one contain. Further data processing to \n",
    "facilitate analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ac_data = pandas.read_csv(\"./data/food_compound.csv\", delimiter = \",\")\n",
    "ac_data.head()\n",
    "\n",
    "# Selecting Useful Anti-Cancer Ingredients Columns\n",
    "\n",
    "ac_data_mod = ac_data[['Common Name', 'Number of CBMs']]\n",
    "ac_data_mod\n",
    "\n",
    "#  Dropping Nan Rows from Anti-Cancer Ingredients Table\n",
    "\n",
    "ac_data_mod.replace(\"\", numpy.nan)\n",
    "ac_data_mod = ac_data_mod.dropna()\n",
    "ac_data_mod\n",
    "\n",
    "# Converting DataFrame to Dictionary\n",
    "\n",
    "ingredient_anticancer = {}\n",
    "\n",
    "for index, row in ac_data_mod.iterrows():\n",
    "    \n",
    "    ingredient_anticancer[row['Common Name'].lower()] = row['Number of CBMs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recipes -> Score\n",
    "\n",
    "Calculating the score of each recipe taking into account the number of cancer-beating molecules. <br>\n",
    "Data Source: Veselkov, K., Gonzalez, G., Aljifri, S. et al. HyperFoods: Machine intelligent mapping of cancer-beating molecules in foods. Sci Rep 9, 9237 (2019) doi:10.1038/s41598-019-45349-y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "recipe_cancerscore = {}\n",
    "recipe_weight = {}\n",
    "\n",
    "for key, value in ingrs_quants_units_final.items():\n",
    "    \n",
    "    recipe_weight[key] = 0\n",
    "    \n",
    "    for recipe_standardized in value:\n",
    "        \n",
    "        recipe_weight[key] = recipe_weight[key] + recipe_standardized[\"quantity (ml)\"]\n",
    "        \n",
    "recipe_weight\n",
    "\n",
    "# ----------------------\n",
    "\n",
    "recipe_cancerscore = {}\n",
    "ingredient_anticancer_keys = list(ingredient_anticancer.keys())\n",
    "\n",
    "for key, value in ingrs_quants_units_final.items():\n",
    "    \n",
    "    recipe_cancerscore[key] = 0\n",
    "    \n",
    "    for recipe_standardized in value:\n",
    "        \n",
    "        for ingredient_anticancer_iterable in ingredient_anticancer_keys:\n",
    "            \n",
    "            if recipe_standardized[\"ingredient\"] in ingredient_anticancer_iterable:\n",
    "        \n",
    "                recipe_cancerscore[key] = recipe_cancerscore[key] + ingredient_anticancer[ingredient_anticancer_iterable]*(recipe_standardized[\"quantity (ml)\"])/(recipe_weight[key])\n",
    "                \n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Recipes Decreasing Order\n",
    "\n",
    "Printing, in a decreasing order, the recipes with a bigger number of cancer-beating molecules. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "res1 = pandas.DataFrame.from_dict(recipe_cancerscore, orient='index', columns=['Anticancer Molecules/Number Ingredients'])\n",
    "res2 = pandas.DataFrame.from_dict(id_url, orient='index', columns=['Recipe URL'])\n",
    "\n",
    "pandas.set_option('display.max_colwidth', 1000)\n",
    "\n",
    "pandas.concat([res1, res2], axis=1).reindex(res1.index).sort_values(by=['Anticancer Molecules/Number Ingredients'], ascending=False).head()\n",
    "\n",
    "# Creating a dataframe object from listoftuples\n",
    "# pandas.DataFrame(recipe_cancerscore_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recipes -> Nutritional Information\n",
    "\n",
    "Retrieving nutritional information for each ingredient present in the recipe dataset. <br>\n",
    "Overall recipe score will be calculated taking into account not only the number of cancer-beating molecules, but also\n",
    "nutrtional content. <br>\n",
    "Data Source: U.S. Department of Agriculture, Agricultural Research Service. FoodData Central, 2019. fdc.nal.usda.gov."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open('./vocabulary/ingr_vocab.pkl', 'rb') as f: # Includes every ingredient present in the dataset.\n",
    "    ingredients_list = pickle.load(f)[1:-1]\n",
    "    \n",
    "print(len(ingredients_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------- Extracting Ingredients\n",
    "\n",
    "new_ingredients_list = [] # List of ingredients from the vocabulary with spaces instead of underscores.\n",
    "\n",
    "for i in range(0, len(ingredients_list)):\n",
    "\n",
    "    if \"_\" in ingredients_list[i]:\n",
    "        new_ingredients_list.append(ingredients_list[i].replace(\"_\", \" \"))\n",
    "        continue\n",
    "\n",
    "    new_ingredients_list.append(ingredients_list[i]) # In case there is no _\n",
    "print(len(new_ingredients_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------- Get FoodData Central IDs for Each Ingredient from Vocab ----------------------------\n",
    "\n",
    "if os.path.exists('./vocabulary/ingredient_fdcIds.json'):\n",
    "    \n",
    "    f = open('./vocabulary/ingredient_fdcIds.json')\n",
    "    ingredient_fdcIds = (json.load(f))# [0:100]\n",
    "    f.close()\n",
    "                \n",
    "else:\n",
    "    \n",
    "    API_Key = \"BslmyYzNnRTysPWT3DDQfNv5lrmfgbmYby3SVsHw\"\n",
    "    URL = \"https://api.nal.usda.gov/fdc/v1/search?api_key=\" + API_Key\n",
    "    \n",
    "    ingredient_fdcIds = {}\n",
    "    \n",
    "    for value in new_ingredients_list:\n",
    "        \n",
    "        ingredient_fdcIds[value] = {}\n",
    "        ingredient_fdcIds[value][\"fdcIds\"] = []\n",
    "        ingredient_fdcIds[value][\"descriptions\"] = []\n",
    "        \n",
    "        # ------------------------------------------ ADDING RAW\n",
    "        PARAMS2 = {'generalSearchInput': value + \" raw\"}\n",
    "        r2 = requests.get(url = URL, params = PARAMS2)\n",
    "        data2 = r2.json()\n",
    "        \n",
    "        raw = False\n",
    "        \n",
    "        if \"foods\" in data2 and value + \" raw\" in (data2[\"foods\"][0][\"description\"]).lower().replace(\",\", \"\"):\n",
    "            \n",
    "            raw_id = data2[\"foods\"][0][\"fdcId\"]\n",
    "            raw_description = data2[\"foods\"][0][\"description\"]\n",
    "            \n",
    "            ingredient_fdcIds[value][\"fdcIds\"].append(raw_id)\n",
    "            ingredient_fdcIds[value][\"descriptions\"].append(raw_description)\n",
    "            \n",
    "            raw = True\n",
    "        \n",
    "        # id_nutritionalInfo[value] = []\n",
    "        \n",
    "        # for i in range(len(value)):\n",
    "        # Defining a params dict for the parameters to be sent to the API \n",
    "        PARAMS = {'generalSearchInput': value} \n",
    "        \n",
    "        # Sending get request and saving the response as response object \n",
    "        r = requests.get(url = URL, params = PARAMS)\n",
    "        \n",
    "        # Extracting data in json format \n",
    "        data = r.json() \n",
    "        \n",
    "        if \"foods\" in data:\n",
    "            \n",
    "            numberMatches = len(data[\"foods\"])\n",
    "            \n",
    "            if numberMatches > 10 and raw == True:\n",
    "                numberMatches = 9\n",
    "            elif numberMatches > 10 and raw == False:\n",
    "                numberMatches = 10\n",
    "            \n",
    "            for i in range(numberMatches):\n",
    "                \n",
    "                ingredient_fdcIds[value][\"fdcIds\"].append(data[\"foods\"][i][\"fdcId\"])\n",
    "                ingredient_fdcIds[value][\"descriptions\"].append(data[\"foods\"][i][\"description\"])\n",
    "                \n",
    "#print(ingredient_fdcIds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------- Get All Nutritional Info from Vocab ----------------------------\n",
    "\n",
    "if os.path.exists('./vocabulary/ingredient_nutritionalInfo.json'):\n",
    "    \n",
    "    f = open('./vocabulary/ingredient_nutritionalInfo.json')\n",
    "    ingredient_nutritionalInfo = (json.load(f))# [0:100]\n",
    "    f.close()\n",
    "                    \n",
    "else:\n",
    "\n",
    "    API_Key = \"BslmyYzNnRTysPWT3DDQfNv5lrmfgbmYby3SVsHw\"\n",
    "    \n",
    "    ingredient_nutritionalInfo = {}\n",
    "    \n",
    "    for key, value in ingredient_fdcIds.items():\n",
    "        \n",
    "        if value[\"fdcIds\"]:\n",
    "        \n",
    "            URL = \"https://api.nal.usda.gov/fdc/v1/\" + str(value[\"fdcIds\"][0]) + \"?api_key=\" + API_Key\n",
    "\n",
    "            # Sending get request and saving the response as response object \n",
    "            r = requests.get(url = URL)\n",
    "\n",
    "            ingredient_nutritionalInfo[key] = {}\n",
    "            ingredient_nutritionalInfo[key][\"fdcId\"] = value[\"fdcIds\"][0]\n",
    "            ingredient_nutritionalInfo[key][\"description\"] = value[\"descriptions\"][0]\n",
    "            ingredient_nutritionalInfo[key][\"nutrients\"] = {}\n",
    "\n",
    "            for foodNutrient in r.json()[\"foodNutrients\"]:\n",
    "\n",
    "                if \"amount\" in foodNutrient.keys():\n",
    "\n",
    "                    ingredient_nutritionalInfo[key][\"nutrients\"][foodNutrient[\"nutrient\"][\"name\"]] = [foodNutrient[\"amount\"], foodNutrient[\"nutrient\"][\"unitName\"]]\n",
    "\n",
    "                else:\n",
    "\n",
    "                    ingredient_nutritionalInfo[key][\"nutrients\"][foodNutrient[\"nutrient\"][\"name\"]] = \"NA\"\n",
    "                    \n",
    "        else:\n",
    "            \n",
    "            ingredient_nutritionalInfo[key] = {}\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------- Correcting Units in JSON with Nutritional Info ----------------------------\n",
    "\n",
    "if os.path.exists('./vocabulary/ingredient_nutritionalInfo_corrected.json'):\n",
    "    \n",
    "    f = open('./vocabulary/ingredient_nutritionalInfo_corrected.json')\n",
    "    ingredient_nutritionalInfo_modified = (json.load(f))# [0:100]\n",
    "    f.close()\n",
    "                    \n",
    "else:\n",
    "\n",
    "    ingredient_nutritionalInfo_modified = ingredient_nutritionalInfo\n",
    "\n",
    "    for nutrient, dictionary in ingredient_nutritionalInfo.items():\n",
    "        \n",
    "        if \"nutrients\" in dictionary:\n",
    "\n",
    "            for molecule, quantity in dictionary[\"nutrients\"].items():\n",
    "\n",
    "                if quantity != \"NA\":\n",
    "\n",
    "                    if quantity[1] == \"mg\":\n",
    "\n",
    "                        ingredient_nutritionalInfo_modified[nutrient][\"nutrients\"][molecule][0] = quantity[0]/1000\n",
    "                        ingredient_nutritionalInfo_modified[nutrient][\"nutrients\"][molecule][1] = 'g'\n",
    "\n",
    "                    elif quantity[1] == \"\\u00b5g\":\n",
    "\n",
    "                        ingredient_nutritionalInfo_modified[nutrient][\"nutrients\"][molecule][0] = quantity[0]/1000000\n",
    "                        ingredient_nutritionalInfo_modified[nutrient][\"nutrients\"][molecule][1] = 'g'\n",
    "\n",
    "                    elif quantity[1] == \"kJ\":\n",
    "\n",
    "                        ingredient_nutritionalInfo_modified[nutrient][\"nutrients\"][molecule][0] = quantity[0]/4.182\n",
    "                        ingredient_nutritionalInfo_modified[nutrient][\"nutrients\"][molecule][1] = 'kcal'\n",
    "\n",
    "                    elif quantity[1] == \"IU\":\n",
    "\n",
    "                        if \"Vitamin A\" in molecule:\n",
    "\n",
    "                            ingredient_nutritionalInfo_modified[nutrient][\"nutrients\"][molecule][0] = quantity[0]*0.45/1000000\n",
    "                            ingredient_nutritionalInfo_modified[nutrient][\"nutrients\"][molecule][1] = 'g'\n",
    "\n",
    "                        elif \"Vitamin C\" in molecule:\n",
    "\n",
    "                            ingredient_nutritionalInfo_modified[nutrient][\"nutrients\"][molecule][0] = quantity[0]*50/1000000\n",
    "                            ingredient_nutritionalInfo_modified[nutrient][\"nutrients\"][molecule][1] = 'g'\n",
    "\n",
    "                        elif \"Vitamin D\" in molecule:\n",
    "\n",
    "                            ingredient_nutritionalInfo_modified[nutrient][\"nutrients\"][molecule][0] = quantity[0]*40/1000000\n",
    "                            ingredient_nutritionalInfo_modified[nutrient][\"nutrients\"][molecule][1] = 'g'\n",
    "\n",
    "                        elif \"Vitamin E\" in molecule:\n",
    "\n",
    "                            ingredient_nutritionalInfo_modified[nutrient][\"nutrients\"][molecule][0] = quantity[0]*0.8/1000\n",
    "                            ingredient_nutritionalInfo_modified[nutrient][\"nutrients\"][molecule][1] = 'g'\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------- Get Medium Sizes for each Ingredient in Vocab ----------------------------\n",
    "\n",
    "f = open('./vocabulary/ingredient_fdcIds.json')\n",
    "ingredient_fdcIds = (json.load(f))#[0:10]\n",
    "f.close()\n",
    "\n",
    "API_Key = \"BslmyYzNnRTysPWT3DDQfNv5lrmfgbmYby3SVsHw\"\n",
    "    \n",
    "ingredient_mediumSize = {}\n",
    "    \n",
    "for key, value in ingredient_fdcIds.items():\n",
    "    \n",
    "    aux = True\n",
    "    \n",
    "    for id_key, fdcId in enumerate(value[\"fdcIds\"][0:5]):\n",
    "        \n",
    "        if not aux:\n",
    "            break\n",
    "        \n",
    "        URL = \"https://api.nal.usda.gov/fdc/v1/\" + str(fdcId) + \"?api_key=\" + API_Key\n",
    "        \n",
    "        # Sending get request and saving the response as response object \n",
    "        r = requests.get(url = URL)\n",
    "        \n",
    "        foodPortions = r.json()[\"foodPortions\"]\n",
    "        i = 0\n",
    "        first_cycle = True\n",
    "        second_cycle = False\n",
    "        third_cycle = False\n",
    "                \n",
    "        while i < len(foodPortions):\n",
    "            \n",
    "            if \"portionDescription\" in foodPortions[i]:\n",
    "                \n",
    "                if \"medium\" in foodPortions[i][\"portionDescription\"] and first_cycle:\n",
    "                            \n",
    "                    ingredient_mediumSize[key] = {\"fdcId\": fdcId, \"description\": value[\"descriptions\"][id_key], \"weight\": foodPortions[i][\"gramWeight\"]}\n",
    "                    aux = False\n",
    "                    break\n",
    "                            \n",
    "                elif i == len(foodPortions) - 1 and first_cycle:\n",
    "                    i = -1\n",
    "                    first_cycle = False\n",
    "                    second_cycle = True\n",
    "                    third_cycle = False\n",
    "                            \n",
    "                elif \"Quantity not specified\" in foodPortions[i][\"portionDescription\"] and second_cycle:\n",
    "                            \n",
    "                    ingredient_mediumSize[key] = {\"fdcId\": fdcId, \"description\": value[\"descriptions\"][id_key], \"weight\": foodPortions[i][\"gramWeight\"]}\n",
    "                    aux = False\n",
    "                    #print(\"Quantity not specified\" + key)\n",
    "                    break\n",
    "                            \n",
    "                elif i == len(foodPortions) - 1 and second_cycle:\n",
    "                    i = -1\n",
    "                    first_cycle = False\n",
    "                    second_cycle = False\n",
    "                    third_cycle = True\n",
    "                            \n",
    "                elif key in foodPortions[i][\"portionDescription\"] and third_cycle:\n",
    "               \n",
    "                    ingredient_mediumSize[key] = {\"fdcId\": fdcId, \"description\": value[\"descriptions\"][id_key], \"weight\": foodPortions[i][\"gramWeight\"]}\n",
    "                    aux = False\n",
    "                    #print(key)\n",
    "                    break\n",
    "                            \n",
    "                elif i == len(foodPortions) - 1 and third_cycle:\n",
    "                    i = -1 \n",
    "                    ingredient_mediumSize[key] = {\"fdcId\": \"NA\", \"description\": \"NA\", \"weight\": \"NA\"}\n",
    "                    first_cycle = False\n",
    "                    second_cycle = False\n",
    "                    third_cycle = False\n",
    "                    break\n",
    "            else:\n",
    "                \n",
    "                break\n",
    "                \n",
    "            i = i + 1    \n",
    "                \n",
    "#print(ingredient_mediumSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------- Save JSON File with Nutritional Info ----------------------------\n",
    "\n",
    "with open('./vocabulary/id_ingredients_cuisine.json', 'w') as json_file:\n",
    "    json.dump(id_ingredients_cuisine, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recipes -> Cuisines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Kaggle and Nature Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96250\n"
     ]
    }
   ],
   "source": [
    "#data = pandas.read_csv(\"./data/jaan/kaggle_and_nature.csv\", skiprows=5)\n",
    "#pandas.read_table('./data/jaan/kaggle_and_nature.csv')\n",
    "#data.head()\n",
    "\n",
    "id_ingredients_cuisine = []\n",
    "cuisines = []\n",
    "        \n",
    "with open('./data/jaan/kaggle_and_nature.csv', newline = '') as games:      \n",
    "    \n",
    "    game_reader = csv.reader(games, delimiter='\\t')\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    for game in game_reader:\n",
    "                \n",
    "        id_ingredients_cuisine.append({\"id\": i, \"ingredients\": [ingredient.replace(\"_\", \" \") for ingredient in game[0].split(\",\")[1:]], \"cuisine\": game[0].split(\",\")[0]})\n",
    "        cuisines.append(game[0].split(\",\")[0])\n",
    "        \n",
    "        i = i + 1\n",
    "        \n",
    "print(len(cuisines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Synonymous Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "881\n",
      "1488\n",
      "881\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------- Importing Recipe1M+ Vocabulary ----------------------------\n",
    "\n",
    "with open('./vocabulary/ingr_vocab.pkl', 'rb') as f: # Includes every ingredient present in the dataset.\n",
    "    ingredients_list = pickle.load(f)\n",
    "    \n",
    "#print(len(ingredients_list))\n",
    "\n",
    "# ---------------------------- Creating Vocabulary to Kaggle and Nature Dataset----------------------------\n",
    "\n",
    "vocabulary = set()\n",
    "\n",
    "for recipe in id_ingredients_cuisine:\n",
    "    \n",
    "    for ingredient in recipe[\"ingredients\"]:\n",
    "        \n",
    "        vocabulary.add(ingredient.replace(\" \", \"_\"))\n",
    "\n",
    "#print(vocabulary)\n",
    "print(len(vocabulary))\n",
    "print(len(ingredients_list))\n",
    "\n",
    "synonymous = {}\n",
    "\n",
    "for ingredient2 in list(vocabulary):\n",
    "    \n",
    "    synonymous[ingredient2] = \"new\"\n",
    "\n",
    "aux = 0\n",
    "\n",
    "for ingredient2 in list(vocabulary):\n",
    "\n",
    "    for ingredient1 in ingredients_list:\n",
    "       \n",
    "        if ingredient1 == ingredient2:\n",
    "            #print(ingredient2 + \" \" + ingredient1)\n",
    "            synonymous[ingredient2] = ingredient1\n",
    "            break\n",
    "            \n",
    "        elif ingredient1 in ingredient2:\n",
    "            \n",
    "            synonymous[ingredient2] = ingredient1\n",
    "            \n",
    "    if synonymous[ingredient2] == \"new\":\n",
    "        aux = aux + 1\n",
    "        \n",
    "print(len(synonymous))\n",
    "\n",
    "new_id_ingredients_cuisine = id_ingredients_cuisine\n",
    "            \n",
    "for key1, recipe in enumerate(id_ingredients_cuisine):\n",
    "    \n",
    "    for key2, ingredient in enumerate(recipe[\"ingredients\"]):\n",
    "        \n",
    "        if synonymous[id_ingredients_cuisine[key1][\"ingredients\"][key2].replace(\" \", \"_\")] == \"new\":\n",
    "            \n",
    "            new_id_ingredients_cuisine[key1][\"ingredients\"].remove(id_ingredients_cuisine[key1][\"ingredients\"][key2])\n",
    "            continue\n",
    "        \n",
    "        new_id_ingredients_cuisine[key1][\"ingredients\"][key2] = synonymous[id_ingredients_cuisine[key1][\"ingredients\"][key2].replace(\" \", \"_\")]   \n",
    "        \n",
    "    if len(id_ingredients_cuisine[key1][\"ingredients\"]) < 2:\n",
    "        new_id_ingredients_cuisine.remove(id_ingredients_cuisine[key1])\n",
    "#print(len(synonymous))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingredients and Recipes to Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1488\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------- Converting Ingredients to Vectors ----------------------------\n",
    "\n",
    "#ingredients = set()\n",
    "\n",
    "#for key, recipe in enumerate(new_id_ingredients_cuisine):\n",
    "    \n",
    "    #for key2, ingredient in enumerate(recipe[\"ingredients\"]):\n",
    "    \n",
    "        #ingredients.add(recipe[\"ingredients\"][key2])\n",
    "    \n",
    "#ingredient_list = ingredients\n",
    "\n",
    "ingredient_list = ingredients_list\n",
    "\n",
    "print(len(ingredient_list))\n",
    "\n",
    "ingredient_vector = {}\n",
    "\n",
    "for key, value in enumerate(ingredient_list):\n",
    "    \n",
    "    ingredient_vector[value] = [0] * len(ingredient_list)\n",
    "    ingredient_vector[value][key] = 1\n",
    "    \n",
    "#print(ingredient_vector[\"cinnamon\"])\n",
    "\n",
    "# ---------------------------- Converting Recipes to Vectors ----------------------------\n",
    "\n",
    "id_ingredients_cuisine_vectorized = {}\n",
    "\n",
    "# print(len(id_ingredients_cuisine))\n",
    "\n",
    "for key1, recipe in enumerate(new_id_ingredients_cuisine[0:20000]):\n",
    "    \n",
    "    id_ingredients_cuisine_vectorized[key1] = []\n",
    "    \n",
    "    for ingredient in recipe[\"ingredients\"]:\n",
    "        \n",
    "        id_ingredients_cuisine_vectorized[key1].append(ingredient_vector[ingredient])\n",
    "        \n",
    "    id_ingredients_cuisine_vectorized[key1] = numpy.sum(numpy.array(id_ingredients_cuisine_vectorized[key1]), 0)\n",
    "    \n",
    "#print(id_ingredients_cuisine_vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Classifier (Linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "          intercept_scaling=1, loss='squared_hinge', max_iter=5000,\n",
       "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ---------------------------- Importing Data ----------------------------\n",
    "\n",
    "X = list(id_ingredients_cuisine_vectorized.values())\n",
    "y = cuisines[0:20000]\n",
    "\n",
    "#for vector in list(id_ingredients_cuisine_vectorized.values()):\n",
    "    #print(len(vector))\n",
    "\n",
    "# ---------------------------- Creating Training & Testing Sets ----------------------------\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "#print(X_train[0:10])\n",
    "#print(y_train[0:10])\n",
    "\n",
    "clf = svm.LinearSVC(max_iter = 5000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# ---------------------------- Save Model ----------------------------\n",
    "\n",
    "#filename = './trained_models/finalized_model2.sav'\n",
    "#pickle.dump(clf, open(filename, 'wb'))\n",
    "\n",
    "# ---------------------------- Load Model ----------------------------\n",
    "\n",
    "#loaded_model = pickle.load(open(filename, 'rb'))\n",
    "# result = loaded_model.score(X_test, Y_test)\n",
    "\n",
    "#print(id_ingredients_cuisine_vectorized[\"10\"])\n",
    "\n",
    "#print(clf.predict([id_ingredients_cuisine_vectorized[430]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# ---------------------------- Importing Data ----------------------------\n",
    "\n",
    "X = list(id_ingredients_cuisine_vectorized.values())\n",
    "y = cuisines[0:20000]\n",
    "\n",
    "# ---------------------------- Creating Training & Testing Sets ----------------------------\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# ---------------------------- Save Model ----------------------------\n",
    "\n",
    "filename = './trained_models/randomForestClassifier.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))\n",
    "\n",
    "# ---------------------------- Load Model ----------------------------\n",
    "\n",
    "#loaded_model = pickle.load(open(filename, 'rb'))\n",
    "# result = loaded_model.score(X_test, Y_test)\n",
    "\n",
    "#print(id_ingredients_cuisine_vectorized[\"10\"])\n",
    "\n",
    "#print(clf.predict([id_ingredients_cuisine_vectorized[430]]))\n",
    "\n",
    "#loaded_model = pickle.load(open(filename, 'rb'))\n",
    "print(clf.predict([id_ingredients_cuisine_vectorized[430]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsides: intuitive and easy to perform.\n",
    "# Downsides: drastically reduce the number of samples which can be used for learning the model, and the results can depend on a particular random choice for the pair of (train, validation) sets.\n",
    "\n",
    "print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stratified K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.80 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "scores = cross_val_score(clf, X_test, y_test, cv=cv)\n",
    "\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leave One Out Cross Validation (LOOCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-936e4b855cf1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLeaveOneOut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy: %0.2f (+/- %0.2f)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/HyperFoods/venv/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    387\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m                                 error_score=error_score)\n\u001b[0m\u001b[1;32m    390\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/HyperFoods/venv/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             error_score=error_score)\n\u001b[0;32m--> 235\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/HyperFoods/venv/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1005\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/HyperFoods/venv/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    833\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/HyperFoods/venv/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/HyperFoods/venv/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/HyperFoods/venv/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/HyperFoods/venv/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/HyperFoods/venv/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/HyperFoods/venv/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/HyperFoods/venv/lib/python3.6/site-packages/sklearn/svm/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m             self.loss, sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"crammer_singer\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/HyperFoods/venv/lib/python3.6/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[0;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[1;32m    937\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m         \u001b[0mclass_weight_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 939\u001b[0;31m         epsilon, sample_weight)\n\u001b[0m\u001b[1;32m    940\u001b[0m     \u001b[0;31m# Regarding rnd.randint(..) in the above signature:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m     \u001b[0;31m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "# LOO is more computationally expensive than k-fold cross validation.\n",
    "\n",
    "cv = LeaveOneOut()\n",
    "\n",
    "scores = cross_val_score(clf, X_test, y_test, cv=cv)\n",
    "\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Cuisine to Recipe1M+ Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69721"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------------------------- Importing Dataset ----------------------------\n",
    "\n",
    "f = open('./data/recipe1M+/layer11.json')\n",
    "recipes_data = (json.load(f))#[0:100000]\n",
    "f.close()\n",
    "\n",
    "# ---------------------------- Converting Ingredients to Vectors ----------------------------\n",
    "modified_recipes_data = {}\n",
    "\n",
    "#print(new_id_ingredients_tokenized)\n",
    "   \n",
    "for key1, list_ingredients in new_id_ingredients_tokenized.items():\n",
    "    \n",
    "    modified_recipes_data[key1] = []\n",
    "    \n",
    "    for key2, ingredient in enumerate(list_ingredients):\n",
    "\n",
    "        modified_recipes_data[key1].append(ingredient_vector[ingredient.replace(\" \", \"_\")])\n",
    "        \n",
    "# ---------------------------- Converting Recipes to Vectors ----------------------------\n",
    "\n",
    "id_ingredients_cuisine_vectorized = {}\n",
    "cuisines_recipe1m = []\n",
    "\n",
    "for key1, recipe in modified_recipes_data.items():\n",
    "            \n",
    "    id_ingredients_cuisine_vectorized[key1] = numpy.sum(numpy.array(modified_recipes_data[key1]), 0)\n",
    "    \n",
    "    cuisines_recipe1m.append((clf.predict([id_ingredients_cuisine_vectorized[key1]]))[0])\n",
    "    \n",
    "# ---------------------------- Adding Cuisines to Recipe1M+ Dataset ----------------------------\n",
    "        \n",
    "modified_modified_recipes_data = recipes_data\n",
    "    \n",
    "for key, recipe in enumerate(recipes_data):\n",
    "    \n",
    "    modified_modified_recipes_data[key][\"cuisine\"] = cuisines_recipe1m[key]\n",
    "    \n",
    "# ---------------------------- Generating New Recipe1M+ w/ Cuisines File ----------------------------\n",
    "    \n",
    "file = open('./data/layer11_modified_cuisines.txt','w') \n",
    "file.write(str(modified_modified_recipes_data))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_ingredients = list(id_ingredients_cuisine_vectorized.values())\n",
    "\n",
    "#print(X_ingredients)\n",
    "\n",
    "# ---------------------------- PCA ----------------------------\n",
    "X_ingredients_embedded1 = PCA(n_components=2).fit_transform(X_ingredients)\n",
    "\n",
    "# ---------------------------- T-SNE ----------------------------\n",
    "# X_ingredients_embedded2 = TSNE(n_components=2).fit_transform(X_ingredients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Calculating Amount of Ingredients & Identifying Recipes' Cuisines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "#recipeModules = [0] * len(list(id_ingredients_cuisine_vectorized.keys()))\n",
    "\n",
    "cuisine_number = {}\n",
    "cuisine_numberized = []\n",
    "index = 0\n",
    "\n",
    "cuisine_number[\"African\"] = 0\n",
    "\n",
    "for key, cuisine in enumerate(cuisines):\n",
    "    \n",
    "    if cuisine not in list(cuisine_number.keys()):\n",
    "    \n",
    "        index = index + 1\n",
    "    \n",
    "        cuisine_number[cuisine] = index\n",
    "\n",
    "for key, cuisine in enumerate(cuisines):\n",
    "    \n",
    "    cuisine_numberized.append(cuisine_number[cuisine])\n",
    "\n",
    "recipeModules = cuisine_numberized\n",
    "\n",
    "print(recipeModules)\n",
    "\n",
    "numberIngredients = [5] * len(list(id_ingredients_cuisine_vectorized.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  PCA & T-SNE Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------- Matplotlib ----------------------------\n",
    "matplotlib_function(X_ingredients_embedded1, X_ingredients_embedded1, list(id_ingredients_cuisine_vectorized.keys()), recipeModules, numberIngredients, \"Recipes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Facebook Recipe Retrieval Algorithm\n",
    "\n",
    "It was created a dictionary object (id_url.json) that matches recipes IDs (layer1.json) with the URLs of images available in layer2.json. While\n",
    "some recipes do not contain images, others contain more than 1. This matching between different files was possible once layer2.json\n",
    "also contain the recipe ID present in layer1.json. \n",
    "\n",
    "Then, by manipulating Facebook's algorithm and its repository, the recipe retrieval algorithm is able to convert the JSON file id_url.json into\n",
    "an array of strings of URLs. Along with this, it creates a parallel array of strings of the IDs of the recipes, so that in each position there is\n",
    "correspondence between ID in this object with an image URL in the previous.\n",
    "\n",
    "Finally, Facebook's algorithm was run and the ingredients list for each image URL was obtained. The number of correct elements over the total\n",
    "number of elements in the ground-truth recipe gives us the accuracy of the algorithm. The ingredients present in each ground-truth recipe \n",
    "were retrieved using the method above - \"Recipe Retrieval w/ Higher Number Anti-Cancer Molecules\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Writing Input File w/ Images to Facebook's Algorithm\n",
    "\n",
    "A JSON file (id_url.json) was created to be input in the Facebook's recipe retrieval algorithm, so that it could generate a prediction of the ingredients \n",
    "present in every recipe from the dataset (with, at least, 1 image available). <br>\n",
    "Ground-truth ingredients for each recipe can be found in layer1.json. The respective images are present in the layer2.json.\n",
    "Both files are in the data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ids = []\n",
    "\n",
    "for recipe in recipes_data:\n",
    "                        \n",
    "    ids.append(recipe[\"id\"])\n",
    "\n",
    "f = open('./data/recipe1M+/layer2.json')\n",
    "recipes_images_data = (json.load(f))# [0:100]\n",
    "f.close()\n",
    "\n",
    "id_images = {}\n",
    "\n",
    "for recipe in recipes_data:\n",
    "\n",
    "    id_images[recipe[\"id\"]] = []\n",
    "    \n",
    "    for recipe_image in recipes_images_data:\n",
    "        \n",
    "        for image in recipe_image[\"images\"]:\n",
    "        \n",
    "            if recipe[\"id\"] == recipe_image[\"id\"]:\n",
    "            \n",
    "                id_images[recipe[\"id\"]].append(image[\"url\"])\n",
    "                    \n",
    "# Writing text file with IDs of each recipe and respective URLs for 1 or more online images.\n",
    "with open('./data/id_url.json', 'w') as json_file:\n",
    "    json.dump(id_images, json_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executing Inverse Cooking Algorithm\n",
    "\n",
    "Recipe Generation from Food Images. </br>\n",
    "https://github.com/facebookresearch/inversecooking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "from demo import demo_func\n",
    "import json\n",
    "\n",
    "f = open('./data/recipe1M+/id_url.json')\n",
    "id_url = (json.load(f))# [0:100]\n",
    "f.close()\n",
    "\n",
    "urls_output = []\n",
    "ids_output = []\n",
    "\n",
    "for id, urls in id_url.items():\n",
    "\n",
    "    for url in urls:\n",
    "\n",
    "        urls_output.append(url)\n",
    "\n",
    "        if url:\n",
    "\n",
    "            ids_output.append(id)\n",
    "\n",
    "print(id_url)\n",
    "print(urls_output)\n",
    "print(ids_output)\n",
    "\n",
    "demo_func(urls_output, ids_output)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Comparing Ingredient Prediction w/ Ground Truth\n",
    "\n",
    "IoU and F1 scores are used to compare the prediction of the ingredients made by the Facebook's algorithm with the ones present\n",
    "in the dataset. <br>\n",
    "First, a JSON file with the prediction for each recipe is read. Then, the 2 scores are calculated. Finally, a comparison between \n",
    "the benchmark performed by the algorithm's team and ours is made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "f = open('./data/id_predictedIngredients.json')\n",
    "id_predictedIngredients = (json.load(f))# [0:100]\n",
    "f.close()\n",
    "\n",
    "# ---------------------------- Intersection over Union (IoU) Score / Jaccard Index ----------------------------\n",
    "\n",
    "iou_list = []\n",
    "\n",
    "recipe_ids = id_predictedIngredients.keys\n",
    "\n",
    "for key, value in id_predictedIngredients.items():\n",
    "    \n",
    "    iou_list.append(iou_function(new_id_ingredients_tokenized[key], value))\n",
    "\n",
    "iou = sum(iou_list)/len(iou_list)\n",
    "\n",
    "# ---------------------------- F1 Score ----------------------------\n",
    "\n",
    "f1_list = []\n",
    "\n",
    "for key, value in id_predictedIngredients.items():\n",
    "    \n",
    "    y_true = [new_id_ingredients_tokenized[key]]\n",
    "    y_pred = [value]\n",
    "    \n",
    "    binarizer = MultiLabelBinarizer()\n",
    "    \n",
    "    # In this case, I am considering only the given labels.\n",
    "    binarizer.fit(y_true)\n",
    "    \n",
    "    f1_list.append(f1_score(binarizer.transform(y_true), binarizer.transform(y_pred), average='macro'))\n",
    "    \n",
    "f1 = sum(f1_list)/len(f1_list)\n",
    "\n",
    "# Benchmark Tests Comparison\n",
    "\n",
    "benchmark = {'Method': [\"Ours\", \"Facebook Group\"],\n",
    "        'IoU': [iou, 0.3252],\n",
    "        'F1': [f1, 0.4908]\n",
    "        }\n",
    "\n",
    "df = pandas.DataFrame(benchmark, columns = ['Method', 'IoU', 'F1'])\n",
    "print(df)\n",
    "\n",
    "# Data obtained by the Facebook Research group comparing how their algorithm, a retrieval system and a human perform when \n",
    "# predicting the ingredients present in the food. \n",
    "\n",
    "Image(\"img/iou&f1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotations\n",
    "\n",
    "List Jupyter running sessions: \n",
    "```console\n",
    "jupyter notebook list\n",
    "```\n",
    "\n",
    "Exit Jupyter notebooks:\n",
    "```\n",
    "jupyter notebook stop (8889)\n",
    "```\n",
    "\n",
    "Plot using Matplotlib:\n",
    "https://medium.com/incedge/data-visualization-using-matplotlib-50ffc12f6af2\n",
    "\n",
    "Add large files to github repo:\n",
    "https://git-lfs.github.com/\n",
    "\n",
    "Removing large file from commit:\n",
    "https://help.github.com/en/github/authenticating-to-github/removing-sensitive-data-from-a-repository\n",
    "https://rtyley.github.io/bfg-repo-cleaner/\n",
    "https://towardsdatascience.com/uploading-large-files-to-github-dbef518fa1a\n",
    "$ bfg --delete-files YOUR-FILE-WITH-SENSITIVE-DATA\n",
    "bfg is an alias for:\n",
    "java -jar bfg.jar\n",
    "\n",
    "Initialize github repo:\n",
    "git init\n",
    "git remote add origin https://gitlab.com/Harmelodic/MyNewProject.git\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "HTML('<iframe src=http://fperez.org/papers/ipython07_pe-gr_cise.pdf width=700 height=350></iframe>')\n",
    "\n",
    "# embbeddidng projector"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
